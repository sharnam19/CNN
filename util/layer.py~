import numpy as np
def eval_numerical_gradient_array(f,x,dOut,h=1e-5):
    grad = np.zeros_like(x)
    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    while not it.finished:
        ix = it.multi_index

        x[ix]+=h
        fxph = f(x).copy()
        x[ix]-=2*h
        fxmh = f(x).copy()
        grad[ix]=np.sum((fxph-fxmh)*dOut)/(2*h)
        x[ix]+=h
        it.iternext()
    return grad
        
def affine_forward(x, w, b):
    """
        Input
        x: Input of shape (N,D)
        w: Weights of shape (D,H)
        b: Biases of shape (H,)

        Output(out,cache)
        Returns out of shape (N,H)

    """
    out = x.dot(w)+b
    cache = (x,w,b)
    return out,cache

def affine_backward(dOut,cache):
    """
        Input:
        dOut: Upstream gradient of shape (N,H)
        Returns Derivative wrt the inputs x,w,b i.e
        dx,dw,db
    """
    x,w,b = cache
    dx = np.dot(dOut,w.T)
    dw = np.dot(x.T,dOut)
    db = np.sum(dOut,axis=0)
        
    return dx,dw,db
    
def relu_forward(x):
    """
    Input : x of any shape
    Output: Of same shape as x after applying Relu Non Linearity
    """
    out = np.maximum(0,x)
    cache = (x)
    return out,cache

def relu_backward(dOut):
    """
    Input: dOut of any shape is the upstream gradient
    Output:dx of same shape as x
    """
    dx = np.sign(np.maximum(0,x))*dOut
    return dx

def sigmoid_forward(x):
    """
    Input x of any shape

    Output: Sigmoided x
    """
    out = 1+np.exp(-x)
    out = 1/out
    cache = (x,out)
    return out

def sigmoid_backward(dOut):
    """
    Input:
    dOut of any shape

    Output:
    dx of same shape as dOut
    """
    _, out = cache
    dx = out*(1-out)*dOut
    return dx
